# GPT-1 
## 论文标题
Improving Language Understanding by Generative Pre-Training
使用通用的预训练来提高语言理解能力

## 摘要
在自然语言理解领域，有很多不同的任务，例如文本蕴涵、问题回答、语义相似度评估和文档分类。
数据方面存在的问题是，有大量未标记的文本文件，标好的数据很少，这导致训练模型比较困难。
接下来作者讲怎么解决这个问题, 首先在未标记的数据上面训练一个预训练模型，这个预训模型是一个语言模型。接下来，在有标号的这些子任务上面训练一个分辨的微调模型。

## 导言
第一段讲怎么更好的利用无监督的文本, 作者提到在当时最成功的模型还是词嵌入(word embeddings)模型。

第二段讲使用未标记的文本数据有两个困难：
1. 不知道哪个优化目标函数(optimization objectives)最有效
2. 怎么样有效的把学到的文本的表示(learned representations)传递给下游的子任务

第三段讲本文提出一个半监督方法(semi-supervised)，先用未标记的文本训练一个比较大的语言模型，然后在子任务上进行微调。这种方法也叫自监督学习(self supervised Learning)

第四段讲本文模型基于 Transformer 架构，作者发现相较于 RNN， Transformer 在迁移学习学到的Feature更加的稳健，作者觉得原因可能是 Transformer 里面有更结构化的记忆。




# 参考资料
[^1]: <a id="limu">[跟李沐学AI: GPT，GPT-2，GPT-3 论文精读【论文精读】](https://www.bilibili.com/video/BV1AF411b7xQ/?spm_id_from=333.999.0.0&vd_source=86f975c2afd68dc55284d0ebca36382b)</a>  
